{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43edd19e-5716-4f22-b460-cd1b6c1bb2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee08f63-ce34-4685-847e-2ec0d0066d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<VERSION>\n",
      "altair: 4.1.0, kats: 0.1.0, pandarallel: 1.5.2\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import warnings\n",
    "import sklearn\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kats.utils.time_series_parameter_tuning as tspt\n",
    "from sklearn.metrics import mean_tweedie_deviance\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from kats.consts import SearchMethodEnum, TimeSeriesData\n",
    "from kats.models.prophet import ProphetModel, ProphetParams\n",
    "from kats.utils.backtesters import BackTesterSimple\n",
    "from kats.utils.parameter_tuning_utils import (\n",
    "    get_default_prophet_parameter_search_space,\n",
    "    get_default_var_parameter_search_space,\n",
    ")\n",
    "from kats.models.var import VARModel, VARParams\n",
    "from pandarallel import pandarallel\n",
    "from utils.evaluation import WRMSSEEvaluator\n",
    "from utils.misc import dump_pickle, load_pickle\n",
    "\n",
    "np.random.seed(42)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(f\"<VERSION>\\naltair: {alt.__version__}, kats: 0.1.0, pandarallel: 1.5.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc727fbb-1fed-4b30-a77d-b9cc710335e4",
   "metadata": {},
   "source": [
    "The Kaggle dataset was saved in the local directory `~/data/mofc-sales-forecast` in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ccdac0-f93d-4b8d-bbfc-e9c196790910",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../data/mofc-sales-forecast\"\n",
    "MODEL_PATH = \"models\"\n",
    "TUNE_PARAMS = True\n",
    "\n",
    "calendar = pd.read_csv(os.path.join(DATA_PATH, \"calendar.csv\"))\n",
    "selling_prices = pd.read_csv(os.path.join(DATA_PATH, \"sell_prices.csv\"))\n",
    "# df_train_valid = pd.read_csv(os.path.join(DATA_PATH, \"sales_train_validation.csv\"))\n",
    "df_train_eval = pd.read_csv(os.path.join(DATA_PATH, \"sales_train_evaluation.csv\"))\n",
    "# sample_submission = pd.read_csv(os.path.join(DATA_PATH, \"sample_submission.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4133e0eb-80df-4d60-9fac-c3b4d223e2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305 out of 30490 IDs were selected for a validation, and 3049 out of 30490 IDs were selected for a test.\n"
     ]
    }
   ],
   "source": [
    "key_names = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "date_names = [\"d_\" + str(i) for i in range(1, 1942)]\n",
    "submission_ids = df_train_eval[\"id\"].unique()\n",
    "test_steps = 28\n",
    "\n",
    "valid_sample_ratio = 0.01\n",
    "test_samplle_ratio = 0.1\n",
    "\n",
    "if valid_sample_ratio == 1.0:\n",
    "    valid_sample_ratio = submission_ids\n",
    "else:\n",
    "    valid_sampled_ids = np.random.choice(\n",
    "        submission_ids, round(valid_sample_ratio * len(submission_ids)), replace=False\n",
    "    ).tolist()\n",
    "\n",
    "if test_samplle_ratio == 1.0:\n",
    "    test_sampled_ids = submission_ids\n",
    "else:\n",
    "    test_sampled_ids = np.random.choice(\n",
    "        submission_ids, round(test_samplle_ratio * len(submission_ids)), replace=False\n",
    "    ).tolist()\n",
    "    \n",
    "print(\n",
    "    f\"{len(valid_sampled_ids)} out of {len(submission_ids)} IDs were selected for a validation, and {len(test_sampled_ids)} out of {len(submission_ids)} IDs were selected for a test.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecab57d5-24c8-431d-861f-c7e0e5ba6391",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "984f782f-1342-45fb-ba56-c7d2c508b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_eval[key_names[:1] + date_names[:-test_steps]]\n",
    "df_train = df_train.set_index(\"id\").T.reset_index()\n",
    "date_dict = calendar[[\"date\", \"d\"]].set_index(\"d\").to_dict()[\"date\"]\n",
    "df_train[\"index\"] = df_train[\"index\"].replace(date_dict)\n",
    "df_train.columns = [\"time\"] + df_train.columns[1:].tolist()\n",
    "df_train.index.name = \"\"\n",
    "\n",
    "series_time = df_train[\"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b877f48-12f0-4c90-bcca-8cc8b436fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = calendar[[\"event_name_1\", \"event_name_2\"]].dropna(how=\"all\").index\n",
    "holidays = calendar.loc[indices, [\"date\", \"event_name_1\", \"event_name_2\"]]\n",
    "holidays = (\n",
    "    pd.melt(\n",
    "        holidays,\n",
    "        id_vars=\"date\",\n",
    "        value_vars=[\"event_name_1\", \"event_name_2\"],\n",
    "        value_name=\"holiday\",\n",
    "    )\n",
    "    .dropna()[[\"holiday\", \"date\"]]\n",
    "    .sort_values(\"date\")\n",
    ")\n",
    "holidays.index = range(holidays.shape[0])\n",
    "holidays.columns = [\"holiday\", \"ds\"]\n",
    "holidays[\"lower_window\"] = 0\n",
    "holidays[\"upper_window\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2248b7d8-a2e2-4251-a4ed-f204cd8a7b12",
   "metadata": {},
   "source": [
    "# Baseline: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26597582-1749-4af2-b3e0-6e2e6bc6ed81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0fef700b3cd4b2fae97f7fadf0261c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampled_df_train_eval = df_train_eval.set_index(\"id\").loc[test_sampled_ids].reset_index()\n",
    "sampled_df_train = sampled_df_train_eval.loc[:, key_names + date_names[:-test_steps]]\n",
    "sampled_df_test = sampled_df_train_eval.loc[:, date_names[-test_steps:]]\n",
    "\n",
    "evaluator = WRMSSEEvaluator(sampled_df_train, sampled_df_test, calendar, selling_prices, test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f11e1343-f160-44d8-aa8e-cd94d34c15a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Method WRMSSE: 1.532600\n"
     ]
    }
   ],
   "source": [
    "sampled_df_pred = pd.DataFrame(\n",
    "    np.repeat(\n",
    "        sampled_df_train[date_names[:-test_steps]].mean(axis=1).values.reshape(-1, 1),\n",
    "        test_steps,\n",
    "        axis=1,\n",
    "    )\n",
    ")\n",
    "sampled_df_pred.columns = sampled_df_test.columns\n",
    "wrmsse = evaluator.score(sampled_df_pred)\n",
    "\n",
    "print(f\"Mean Method WRMSSE: {wrmsse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d582a5ac-b65b-4bfb-9aa4-6d5f89e47efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Method WRMSSE: 1.393950\n"
     ]
    }
   ],
   "source": [
    "sampled_df_pred = pd.DataFrame(\n",
    "    np.repeat(\n",
    "        sampled_df_train[date_names[-test_steps - 1 : -test_steps]].values,\n",
    "        test_steps,\n",
    "        axis=1,\n",
    "    )\n",
    ")\n",
    "sampled_df_pred.columns = sampled_df_test.columns\n",
    "wrmsse = evaluator.score(sampled_df_pred)\n",
    "\n",
    "print(f\"Naive Method WRMSSE: {wrmsse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f17d257-1abf-4ac5-bd41-f3f9b781453a",
   "metadata": {},
   "source": [
    "# Prophet: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24ab9759-b854-44f2-86f3-92a7f7f27048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict_model(df, steps, params, include_history=False):\n",
    "    sales_ts = TimeSeriesData(df=df, sort_by_time=True, time_col_name=\"time\")\n",
    "\n",
    "    model = ProphetModel(data=sales_ts, params=params)\n",
    "    model.fit()\n",
    "\n",
    "    forecast = model.predict(\n",
    "        steps=steps,\n",
    "        include_history=include_history,\n",
    "        freq=\"D\",\n",
    "    )\n",
    "\n",
    "    return model, forecast\n",
    "    \n",
    "\n",
    "def get_func(prophet_params, train_index, test_index):\n",
    "    def calc_model_loss(y):\n",
    "        global series_time\n",
    "        eps = 1e-6\n",
    "        \n",
    "        df = pd.concat([series_time, y], axis=1)\n",
    "        df.columns = [\"time\", \"y\"]\n",
    "        \n",
    "        model, forecast = fit_and_predict_model(\n",
    "            df.loc[train_index, :], \n",
    "            len(test_index),\n",
    "            prophet_params\n",
    "        )\n",
    "\n",
    "        y_true = df.loc[test_index, :].iloc[:, 1].values\n",
    "        y_pred = forecast[\"fcst\"].values\n",
    "        y_pred = np.where(y_pred < eps, eps, y_pred)\n",
    "\n",
    "        return mean_tweedie_deviance(y_true, y_pred, power=1.5)\n",
    "\n",
    "    return calc_model_loss\n",
    "\n",
    "\n",
    "def evaluation_function(params):\n",
    "    prophet_params = ProphetParams(\n",
    "        n_changepoints=params[\"n_changepoints\"],\n",
    "        changepoint_range=params[\"changepoint_range\"],\n",
    "        yearly_seasonality=params[\"yearly_seasonality\"],\n",
    "        weekly_seasonality=params[\"weekly_seasonality\"],\n",
    "        daily_seasonality=params[\"daily_seasonality\"],\n",
    "        holidays=holidays,\n",
    "        holidays_prior_scale=params[\"holidays_prior_scale\"],\n",
    "        seasonality_mode=params[\"seasonality_mode\"],\n",
    "        seasonality_prior_scale=params[\"seasonality_prior_scale\"],\n",
    "        changepoint_prior_scale=params[\"changepoint_prior_scale\"],\n",
    "        floor=0.0,\n",
    "    )\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    losses = []\n",
    "\n",
    "    for train_index, test_index in tscv.split(df_train):\n",
    "        evaluate = get_func(prophet_params, train_index, test_index)\n",
    "        res = df_train[valid_sampled_ids].apply(evaluate)\n",
    "        losses.append(res.mean())\n",
    "\n",
    "    return np.array(losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef87ca2f-d982-459d-9ccb-1b8e23a5145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "if TUNE_PARAMS:\n",
    "    parameters = get_default_prophet_parameter_search_space()\n",
    "    parameters[0][\"values\"] = parameters[0][\"values\"] + [25.0, 50.0, 100.0]\n",
    "    parameters.append(\n",
    "        {\n",
    "            \"name\": \"n_changepoints\",\n",
    "            \"type\": \"choice\",\n",
    "            \"value_type\": \"int\",\n",
    "            \"values\": [5, 10, 25, 50, 100],\n",
    "            \"is_ordered\": True,\n",
    "        }\n",
    "    )\n",
    "    parameters.append(\n",
    "        {\n",
    "            \"name\": \"holidays_prior_scale\",\n",
    "            \"type\": \"choice\",\n",
    "            \"value_type\": \"float\",\n",
    "            \"values\": [0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5, 10.0, 25.0, 50.0, 100.0],\n",
    "            \"is_ordered\": True,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    parameter_tuner = tspt.SearchMethodFactory.create_search_method(\n",
    "        parameters=parameters,\n",
    "        evaluation_function=evaluation_function,\n",
    "        bootstrap_size=10,\n",
    "        selected_search_method=SearchMethodEnum.BAYES_OPT,\n",
    "        seed=42,\n",
    "        multiprocessing=True,\n",
    "    )\n",
    "\n",
    "    parameter_tuner.generate_evaluate_new_parameter_values(\n",
    "        evaluation_function=evaluation_function,\n",
    "        arm_count=20,\n",
    "    )\n",
    "\n",
    "    tuning_results = parameter_tuner.list_parameter_value_scores()\n",
    "    dump_pickle(os.path.join(MODEL_PATH, \"prophet_tuning_results.pkl\"), tuning_results)\n",
    "\n",
    "else:\n",
    "    tuning_results = load_pickle(os.path.join(MODEL_PATH, \"prophet_tuning_results.pkl\"))\n",
    "\n",
    "best_params = tuning_results.loc[tuning_results[\"mean\"].argmin(), \"parameters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f677ecef-86a9-408f-b98d-e9ce07fd48bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.concat(\n",
    "    [pd.json_normalize(tuning_results[\"parameters\"]), tuning_results[\"mean\"]],\n",
    "    axis=1,\n",
    ")\n",
    "summary.columns = summary.columns[:-1].tolist() + [\"loss\"]\n",
    "summary.sort_values(\"loss\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156083e1-d7f1-4856-ad72-03cec248d9a3",
   "metadata": {},
   "source": [
    "# Prophet: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c6a03-dda2-486d-bb9b-682f4802d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_params = ProphetParams(\n",
    "    n_changepoints=best_params[\"n_changepoints\"],\n",
    "    changepoint_range=best_params[\"changepoint_range\"],\n",
    "    yearly_seasonality=best_params[\"yearly_seasonality\"],\n",
    "    weekly_seasonality=best_params[\"weekly_seasonality\"],\n",
    "    daily_seasonality=best_params[\"daily_seasonality\"],\n",
    "    holidays=holidays,\n",
    "    seasonality_mode=best_params[\"seasonality_mode\"],\n",
    "    seasonality_prior_scale=best_params[\"seasonality_prior_scale\"],\n",
    "    holidays_prior_scale=best_params[\"holidays_prior_scale\"],\n",
    "    changepoint_prior_scale=best_params[\"changepoint_prior_scale\"],\n",
    "    floor=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bafecb1-90c1-41c8-8d08-d7c76bca8966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(y):\n",
    "    global series_time\n",
    "    global all_errors\n",
    "    global prophet_params\n",
    "    global train_percentage\n",
    "    global test_percentage\n",
    "    \n",
    "    df = pd.concat([series_time, y], axis=1)\n",
    "    df.columns = [\"time\", \"y\"]\n",
    "    \n",
    "    sales_ts = TimeSeriesData(df=df, sort_by_time=True, time_col_name=\"time\")\n",
    "    \n",
    "    backtester = BackTesterSimple(\n",
    "        train_percentage=train_percentage,\n",
    "        test_percentage=test_percentage, \n",
    "        error_methods=all_errors,\n",
    "        data=sales_ts,\n",
    "        params=prophet_params,\n",
    "        model_class=ProphetModel,\n",
    "    )\n",
    "    \n",
    "    backtester.run_backtest()\n",
    "    \n",
    "    return list(backtester.errors.values())\n",
    "\n",
    "\n",
    "def predict(y):\n",
    "    global series_time\n",
    "    global prophet_params\n",
    "    global test_steps\n",
    "\n",
    "    df = pd.concat([series_time, y], axis=1)\n",
    "    df.columns = [\"time\", \"y\"]\n",
    "    \n",
    "    model, forecast = fit_and_predict_model(df, test_steps, prophet_params)\n",
    "\n",
    "    y_pred = forecast[\"fcst\"].values\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506ab39a-308d-40a7-9d79-56fd528a5b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pandarallel.initialize(\n",
    "    nb_workers=multiprocessing.cpu_count() - 1,\n",
    "    progress_bar=False,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "all_errors = [\"mape\", \"smape\", \"mae\", \"mase\", \"mse\", \"rmse\"]\n",
    "train_percentage = 100 * len(date_names) / (len(date_names) + test_steps)\n",
    "test_percentage = 100 - train_percentage\n",
    "\n",
    "backtests = df_train[test_sampled_ids].parallel_apply(backtest, result_type=\"reduce\")\n",
    "\n",
    "parsed = dict()\n",
    "for index, values in backtests.iteritems():\n",
    "    parsed[index] = list(values)\n",
    "backtests = pd.DataFrame(parsed, index=all_errors).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ad795-5dd4-40ba-893f-cbb454486280",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\n",
    "for key, value in backtests.items():\n",
    "    if not np.isnan(value):\n",
    "        string += key + \": \" + f\"{value:.4f}\\n\"\n",
    "        \n",
    "print(string[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2609843-ac91-49e3-b106-be40f84f6246",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictions = df_train[test_sampled_ids].parallel_apply(predict, result_type=\"reduce\")\n",
    "\n",
    "parsed = dict()\n",
    "for index, values in predictions.iteritems():\n",
    "    parsed[index] = list(values)\n",
    "predictions = pd.DataFrame(parsed).iloc[-test_steps:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132cb72a-8e9a-4c6d-97fe-ec3df4da9d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df_pred = predictions.T\n",
    "sampled_df_pred = sampled_df_pred.loc[test_sampled_ids]\n",
    "sampled_df_pred.columns = sampled_df_test.columns\n",
    "sampled_df_pred.index = range(len(test_sampled_ids))\n",
    "\n",
    "wrmsse = evaluator.score(sampled_df_pred)\n",
    "\n",
    "print(f\"Propeht WRMSSE: {wrmsse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79803e83-62c7-4804-98a0-f2acf494a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecast(source, test_steps, plot_id=None, model_name=None, start_date=None):\n",
    "    if start_date is not None:\n",
    "        source = source[source[\"time\"] >= start_date]\n",
    "\n",
    "    points = (\n",
    "        alt.Chart(source)\n",
    "        .mark_circle(size=10.0, color=\"#000000\")\n",
    "        .encode(\n",
    "            x=alt.X(\"time:T\", axis=alt.Axis(title=\"Date\")),\n",
    "            y=alt.Y(\"y\", axis=alt.Axis(title=\"Sales\")),\n",
    "            tooltip=[\"time:T\", \"y:Q\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    line = (\n",
    "        alt.Chart(source)\n",
    "        .mark_line(size=1.0, color=\"#4267B2\")\n",
    "        .encode(\n",
    "            x=\"time:T\",\n",
    "            y=\"fcst\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    band = (\n",
    "        alt.Chart(source)\n",
    "        .mark_area(opacity=0.25, color=\"#4267B2\")\n",
    "        .encode(\n",
    "            x=\"time:T\",\n",
    "            y=\"fcst_lower\",\n",
    "            y2=\"fcst_upper\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    rule = (\n",
    "        alt.Chart(source[[\"time\"]].iloc[-test_steps : -test_steps + 1])\n",
    "        .mark_rule(size=1.0, color=\"#FF0000\", strokeDash=[2, 2])\n",
    "        .encode(x=\"time:T\")\n",
    "    )\n",
    "\n",
    "    title = \"Sales Forecast\"\n",
    "    if plot_id is not None:\n",
    "        title += f\" for '{plot_id}'\"\n",
    "    if model_name is not None:\n",
    "        title = f\"{model_name}: \" + title\n",
    "\n",
    "    return (points + line + band + rule).properties(title=title, width=1000, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e8333-7ff3-43ca-a6c7-86df99a6b6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_index = 2  # 2, 3, 9\n",
    "plot_id = test_sampled_ids[plot_index]\n",
    "\n",
    "df = df_train[[\"time\"] + [plot_id]]\n",
    "df.columns = [\"time\", \"y\"]\n",
    "\n",
    "model, forecast = fit_and_predict_model(\n",
    "    df, test_steps, prophet_params, include_history=True\n",
    ")\n",
    "\n",
    "y = (df_train_eval[df_train_eval[\"id\"] == plot_id].loc[:, date_names]).T\n",
    "y.columns = [\"y\"]\n",
    "y = calendar.merge(y, left_on=\"d\", right_index=True)[[\"date\", \"y\"]]\n",
    "y[\"time\"] = pd.to_datetime(y[\"date\"])\n",
    "\n",
    "source = y.merge(forecast, how=\"left\").drop([\"date\"], axis=1)\n",
    "plot_forecast(\n",
    "    source, test_steps, plot_id=plot_id, model_name=\"Prophet\", start_date=\"2015-05-23\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2dd827-8bd8-44e5-8f2a-f8dae690e7ca",
   "metadata": {},
   "source": [
    "# VAR: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5375ed-6a46-40e0-b8e8-c53b93140c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "var_params = VARParams(trend=\"ct\")\n",
    "\n",
    "df = pd.concat([series_time, df_train[test_sampled_ids]], axis=1)\n",
    "sales_ts = TimeSeriesData(\n",
    "    df=df, sort_by_time=True, time_col_name=\"time\"\n",
    ")\n",
    "\n",
    "model = VARModel(data=sales_ts, params=var_params)\n",
    "model.fit()\n",
    "\n",
    "forecast = model.predict(\n",
    "    steps=test_steps,\n",
    "    include_history=True,\n",
    "    freq=\"D\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3122fb3b-8849-4508-86ac-d7b9a38bfdf7",
   "metadata": {},
   "source": [
    "* The backtesters module currently only supports univariate, so it was not used for **VAR**, a multivariate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b773a6-78ac-4105-9c1c-264f7d85ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = dict()\n",
    "for test_sampled_id in test_sampled_ids:\n",
    "    parsed[test_sampled_id] = (\n",
    "        forecast[test_sampled_id].to_dataframe()[\"fcst\"].values.tolist()\n",
    "    )\n",
    "predictions = pd.DataFrame(parsed).iloc[-test_steps:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0644d7cd-e950-4e66-81c4-6ee547e91db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df_pred = predictions.T\n",
    "sampled_df_pred = sampled_df_pred.loc[test_sampled_ids]\n",
    "sampled_df_pred.columns = sampled_df_test.columns\n",
    "sampled_df_pred.index = range(len(test_sampled_ids))\n",
    "\n",
    "wrmsse = evaluator.score(sampled_df_pred)\n",
    "\n",
    "print(f\"VAR WRMSSE: {wrmsse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd010aa8-bbb4-402c-9ec6-fd2074d2e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_index = 2  # 2, 3, 9\n",
    "plot_id = test_sampled_ids[plot_index]\n",
    "\n",
    "y = (df_train_eval[df_train_eval[\"id\"] == plot_id].loc[:, date_names]).T\n",
    "y.columns = [\"y\"]\n",
    "y = calendar.merge(y, left_on=\"d\", right_index=True)[[\"date\", \"y\"]]\n",
    "y[\"time\"] = pd.to_datetime(y[\"date\"])\n",
    "\n",
    "source = y.merge(forecast[plot_id].to_dataframe(), how=\"left\").drop([\"date\"], axis=1)\n",
    "\n",
    "plot_forecast(source, test_steps, plot_id=plot_id, model_name=\"VAR\", start_date=\"2015-05-23\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
