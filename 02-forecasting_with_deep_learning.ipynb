{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d59165-02ca-4369-a94e-3e6ec73d65b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e94915-cfe3-41eb-ab51-5b55074eccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import warnings\n",
    "import category_encoders\n",
    "import gluonts\n",
    "import mxnet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder\n",
    "from category_encoders.hashing import HashingEncoder\n",
    "from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
    "from gluonts.evaluation import Evaluator, MultivariateEvaluator\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.model.deepvar import DeepVAREstimator\n",
    "from gluonts.model.predictor import Predictor\n",
    "from gluonts.mx.distribution import (\n",
    "    LowrankMultivariateGaussianOutput,\n",
    "    NegativeBinomialOutput, \n",
    ")\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from mxnet.context import num_gpus\n",
    "from utils.evaluation import calc_eval_metric, WRMSSEEvaluator\n",
    "\n",
    "mxnet.random.seed(42)\n",
    "np.random.seed(42)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3935438-e3c0-41e5-87ab-5c5a64f53b3a",
   "metadata": {},
   "source": [
    "The Kaggle dataset was saved in the local directory `~/data/mofc-demand-forecast` in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ea8ceb-ddf6-49d8-9311-77d26cc5282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../data/mofc-demand-forecast\"\n",
    "MODEL_PATH = \"models\"\n",
    "\n",
    "calendar = pd.read_csv(os.path.join(DATA_PATH, \"calendar.csv\"))\n",
    "selling_prices = pd.read_csv(os.path.join(DATA_PATH, \"sell_prices.csv\"))\n",
    "df_train_valid = pd.read_csv(os.path.join(DATA_PATH, \"sales_train_validation.csv\"))\n",
    "df_train_eval = pd.read_csv(os.path.join(DATA_PATH, \"sales_train_evaluation.csv\"))\n",
    "sample_submission = pd.read_csv(os.path.join(DATA_PATH, \"sample_submission.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738144a6-729f-4e48-8844-8f7d284a8016",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_names = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "all_ids = df_train_eval[\"id\"].unique()\n",
    "date_names = [\"d_\" + str(i) for i in range(1, 1942)]\n",
    "dates = calendar[\"date\"].unique()\n",
    "test_steps = 28\n",
    "\n",
    "key_pairs = list(itertools.product(all_ids, dates))\n",
    "key_pairs = pd.DataFrame(key_pairs, columns=[\"id\", \"date\"])\n",
    "\n",
    "test_sample_ratio = 0.1\n",
    "\n",
    "if test_sample_ratio == 1.0:\n",
    "    test_sampled_ids = all_ids\n",
    "else:\n",
    "    test_sampled_ids = np.random.choice(\n",
    "        all_ids, round(test_sample_ratio * len(all_ids)), replace=False\n",
    "    ).tolist()\n",
    "    \n",
    "print(\n",
    "    f\"{len(test_sampled_ids)} out of {len(all_ids)} IDs were selected for testing.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9edd18-b652-4fed-86e9-97a7cdc40e08",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d39a4d-d7d8-432b-a50f-8d480b250bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_train_eval[[\"id\"] + date_names]\n",
    "target = target.set_index(\"id\").T.reset_index()\n",
    "date_dict = calendar[[\"date\", \"d\"]].set_index(\"d\").to_dict()[\"date\"]\n",
    "target[\"index\"] = target[\"index\"].replace(date_dict)\n",
    "target.columns = [\"date\"] + target.columns[1:].tolist()\n",
    "target = target.set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae6db7f-f6ca-4fb8-8bec-7659a8681bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n",
    "events = calendar[[\"date\"] + feature_names].fillna(\"NA\")\n",
    "train = events[events[\"date\"] < dates[-2 * test_steps]][feature_names]\n",
    "\n",
    "encoder = HashingEncoder(drop_invariant=True)\n",
    "_ = encoder.fit(train)\n",
    "encoded = encoder.transform(events[feature_names])\n",
    "events = pd.concat([events[[\"date\"]], encoded], axis=1)\n",
    "\n",
    "time_related = calendar[[\"date\", \"wday\", \"month\"]]\n",
    "time_related[\"day\"] = time_related[\"date\"].map(lambda x: int(x.split(\"-\")[2]))\n",
    "\n",
    "feat_dynamic_cat = events.merge(time_related).set_index(\"date\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(feat_dynamic_cat)\n",
    "feat_dynamic_cat = pd.DataFrame(\n",
    "    scaled, columns=feat_dynamic_cat.columns, index=feat_dynamic_cat.index\n",
    ")\n",
    "n_feat_dynamic_cat = feat_dynamic_cat.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042273de-a9e6-4422-a7d7-7ce528ef6f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = (\n",
    "    df_train_eval[[\"id\", \"store_id\", \"item_id\"]]\n",
    "    .merge(selling_prices, how=\"left\")\n",
    "    .drop([\"store_id\", \"item_id\"], axis=1)\n",
    ")\n",
    "week_to_date = calendar[[\"date\", \"wm_yr_wk\"]].drop_duplicates()\n",
    "prices = week_to_date.merge(prices, how=\"left\").drop(\n",
    "    [\"wm_yr_wk\"], axis=1\n",
    ")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train = prices[prices[\"date\"] < dates[-2 * test_steps]][[\"sell_price\"]]\n",
    "\n",
    "_ = scaler.fit(train)\n",
    "prices[\"sell_price\"] = scaler.transform(prices[[\"sell_price\"]])\n",
    "prices = prices.pivot(index=\"date\", columns=\"id\", values=\"sell_price\")\n",
    "prices = prices.fillna(method=\"bfill\")\n",
    "\n",
    "snap = calendar[[\"date\", \"snap_CA\", \"snap_TX\", \"snap_WI\"]]\n",
    "snap.columns = [\"date\", \"CA\", \"TX\", \"WI\"]\n",
    "snap = pd.melt(\n",
    "    snap,\n",
    "    id_vars=\"date\",\n",
    "    value_vars=[\"CA\", \"TX\", \"WI\"],\n",
    "    var_name=\"state_id\",\n",
    "    value_name=\"snap\",\n",
    ")\n",
    "snap = key_pairs.merge(df_train_eval[[\"id\", \"state_id\"]], how=\"left\").merge(\n",
    "    snap, on=[\"date\", \"state_id\"], how=\"left\"\n",
    ")\n",
    "snap = snap.pivot(index=\"date\", columns=\"id\", values=\"snap\")\n",
    "\n",
    "feat_dynamic_real = pd.concat([prices, snap], axis=1)\n",
    "n_feat_dynamic_real = int(feat_dynamic_real.shape[1] / target.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5e5832-c6e2-4e0b-874d-4e50006d5af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "feat_static_cat = df_train_eval[[\"id\"] + feature_names]\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "feat_static_cat[feature_names] = encoder.fit_transform(feat_static_cat[feature_names])\n",
    "feat_static_cat[feature_names] = feat_static_cat[feature_names].astype(int)\n",
    "feat_static_cat = feat_static_cat.set_index(\"id\").T\n",
    "\n",
    "cardinality = [len(category) for category in encoder.categories_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e94eeb-ac21-4344-869b-c08b51092fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_n_array(x, n):\n",
    "    return np.hsplit(x.values.T.ravel(), n)\n",
    "\n",
    "\n",
    "train_list = []\n",
    "for test_sampled_id in test_sampled_ids:\n",
    "    dict_by_id = {\n",
    "        FieldName.TARGET: target[test_sampled_id].iloc[:-test_steps].values,\n",
    "        FieldName.START: target.index[0],\n",
    "        FieldName.FEAT_DYNAMIC_REAL: split_into_n_array(\n",
    "            feat_dynamic_cat.iloc[: -2 * test_steps], \n",
    "            n_feat_dynamic_cat,\n",
    "        )\n",
    "        + split_into_n_array(\n",
    "            feat_dynamic_real[test_sampled_id].iloc[: -2 * test_steps],\n",
    "            n_feat_dynamic_real,\n",
    "        ),\n",
    "        FieldName.FEAT_STATIC_CAT: feat_static_cat[test_sampled_id].values,\n",
    "    }\n",
    "    train_list.append(dict_by_id)\n",
    "\n",
    "test_list = []\n",
    "for test_sampled_id in test_sampled_ids:\n",
    "    dict_by_id = {\n",
    "        FieldName.TARGET: target[test_sampled_id].values,\n",
    "        FieldName.START: target.index[0],\n",
    "        FieldName.FEAT_DYNAMIC_REAL: split_into_n_array(\n",
    "            feat_dynamic_cat.iloc[: -test_steps], \n",
    "            n_feat_dynamic_cat,\n",
    "        )\n",
    "        + split_into_n_array(\n",
    "            feat_dynamic_real[test_sampled_id].iloc[: -test_steps],\n",
    "            n_feat_dynamic_real,\n",
    "        ),\n",
    "        FieldName.FEAT_STATIC_CAT: feat_static_cat[test_sampled_id].values,\n",
    "    }\n",
    "    test_list.append(dict_by_id)\n",
    "    \n",
    "train_dataset = ListDataset(train_list, freq=\"D\")\n",
    "test_dataset = ListDataset(test_list, freq=\"D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9b1c0b-04ba-4a34-978b-86b5b924af81",
   "metadata": {},
   "source": [
    "# DeepAR: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedcf2e0-d9af-4f1d-a51b-32fe46581f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"gpu\" if num_gpus() > 0 else \"cpu\"\n",
    "trainer = Trainer(\n",
    "    ctx=device,\n",
    "    epochs=200,\n",
    "    num_batches_per_epoch=50,\n",
    "    learning_rate=0.001,\n",
    "    hybridize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b489c21-badd-4bfa-8407-e0f1cf0f815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepar_estimator = DeepAREstimator(\n",
    "    freq=\"D\", \n",
    "    prediction_length=test_steps,\n",
    "    trainer=trainer,\n",
    "    context_length=2 * test_steps,\n",
    "    num_layers=2,\n",
    "    num_cells=40,\n",
    "    cell_type=\"lstm\",\n",
    "    dropout_rate=0.1,\n",
    "    use_feat_dynamic_real=True,\n",
    "    use_feat_static_cat=True,\n",
    "    use_feat_static_real=False,\n",
    "    cardinality=cardinality,\n",
    "    distr_output=NegativeBinomialOutput(),\n",
    "    batch_size=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037efb06-81ec-4875-b587-1a0b41d28bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "deepar_predictor = deepar_estimator.train(train_dataset)\n",
    "\n",
    "os.makedirs(os.path.join(MODEL_PATH, \"deepar\"), exist_ok=True)\n",
    "deepar_predictor.serialize(Path(os.path.join(MODEL_PATH, \"deepar\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9bab3d-d4b2-4d5b-a0fd-3f307ebfdbe6",
   "metadata": {},
   "source": [
    "# DeepAR: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758f8a4f-48b0-4a2c-978a-b621336b9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepar_predictor = Predictor.deserialize(Path(os.path.join(MODEL_PATH, \"deepar\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33084a2-5bbf-4f4a-841d-5e13c111b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "forecast_iter, ts_iter = make_evaluation_predictions( \n",
    "    dataset=test_dataset,\n",
    "    predictor=deepar_predictor, \n",
    "    num_samples=100,\n",
    ") \n",
    "forecasts = list(forecast_iter)\n",
    "tss = list(ts_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2f5f87-12e2-41ee-bdc9-66f1d6fc5e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_series = len(test_sampled_ids)\n",
    "\n",
    "evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
    "agg_metrics, item_metrics = evaluator(\n",
    "    iter(tss), iter(forecasts), num_series=num_series\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907b3737-3aef-44d6-ae6e-795d7a665582",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\n",
    "for key, value in agg_metrics.items():\n",
    "    if not np.isnan(value):\n",
    "        string += key + \": \" + f\"{value:.4f}\\n\"\n",
    "        \n",
    "print(string[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb414f5d-1229-4581-be1f-3d401b0b862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = (\n",
    "    df_train_eval.set_index(\"id\").loc[test_sampled_ids].reset_index()\n",
    ")\n",
    "df_train_sampled = df_sampled.loc[:, key_names + date_names[:-test_steps]]\n",
    "df_test_sampled = df_sampled.loc[:, date_names[-test_steps:]]\n",
    "\n",
    "wrmsse_evaluator = WRMSSEEvaluator(\n",
    "    df_train_sampled, df_test_sampled, calendar, selling_prices, test_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae0735-1034-432e-aa67-92fab8069ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [forecast.mean for forecast in forecasts]\n",
    "df_pred_sampled = pd.DataFrame(predictions, columns=df_test_sampled.columns)\n",
    "\n",
    "wrmsse = wrmsse_evaluator.score(df_pred_sampled)\n",
    "\n",
    "print(f\"DeepAR WRMSSE: {wrmsse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b53b8ed-43b5-4419-9149-36cf741603ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecast(source, test_steps, plot_id=None, model_name=None, start_date=None):\n",
    "    if start_date is not None:\n",
    "        source = source[source[\"time\"] >= start_date]\n",
    "\n",
    "    points = (\n",
    "        alt.Chart(source)\n",
    "        .mark_circle(size=10.0, color=\"#000000\")\n",
    "        .encode(\n",
    "            x=alt.X(\"time:T\", axis=alt.Axis(title=\"Date\")),\n",
    "            y=alt.Y(\"y\", axis=alt.Axis(title=\"Demand\")),\n",
    "            tooltip=[\"time:T\", \"y:Q\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    line = (\n",
    "        alt.Chart(source)\n",
    "        .mark_line(size=1.0, color=\"#4267B2\")\n",
    "        .encode(\n",
    "            x=\"time:T\",\n",
    "            y=\"fcst\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    band_90 = (\n",
    "        alt.Chart(source)\n",
    "        .mark_area(opacity=0.25, color=\"#4267B2\")\n",
    "        .encode(\n",
    "            x=\"time:T\",\n",
    "            y=\"fcst_lower_05\",\n",
    "            y2=\"fcst_upper_95\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    band_50 = (\n",
    "        alt.Chart(source)\n",
    "        .mark_area(opacity=0.5, color=\"#4267B2\")\n",
    "        .encode(\n",
    "            x=\"time:T\",\n",
    "            y=\"fcst_lower_25\",\n",
    "            y2=\"fcst_upper_75\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    rule = (\n",
    "        alt.Chart(source[[\"time\"]].iloc[-test_steps : -test_steps + 1])\n",
    "        .mark_rule(size=1.0, color=\"#FF0000\", strokeDash=[2, 2])\n",
    "        .encode(x=\"time:T\")\n",
    "    )\n",
    "\n",
    "    title = \"Demand Forecast\"\n",
    "    if plot_id is not None:\n",
    "        title += f\" for '{plot_id}'\"\n",
    "    if model_name is not None:\n",
    "        title = f\"{model_name}: \" + title\n",
    "\n",
    "    return (points + line + band_90 + band_50 + rule).properties(title=title, width=1000, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0319af-1f7a-4617-ad8a-8ad635f8b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_indices = [2, 4, 8]\n",
    "plots = []\n",
    "\n",
    "for plot_index in plot_indices:\n",
    "    plot_id = test_sampled_ids[plot_index]\n",
    "\n",
    "    y = target[[plot_id]].reset_index()\n",
    "    y.columns = [\"time\", \"y\"]\n",
    "    y[\"time\"] = pd.to_datetime(y[\"time\"])\n",
    "\n",
    "    forecast = pd.DataFrame(\n",
    "        [forecasts[plot_index].mean]\n",
    "        + [forecasts[plot_index].quantile(p) for p in [0.05, 0.25, 0.75, 0.95]],\n",
    "        columns=forecasts[plot_index].index,\n",
    "    ).T.reset_index()\n",
    "    forecast.columns = [\n",
    "        \"time\",\n",
    "        \"fcst\",\n",
    "        \"fcst_lower_05\",\n",
    "        \"fcst_lower_25\",\n",
    "        \"fcst_upper_75\",\n",
    "        \"fcst_upper_95\",\n",
    "    ]\n",
    "\n",
    "    source = y.merge(forecast, how=\"left\")\n",
    "    p = plot_forecast(\n",
    "        source, test_steps, plot_id=plot_id, model_name=\"DeepAR\", start_date=\"2015-05-23\"\n",
    "    )\n",
    "    \n",
    "    plots.append(p)\n",
    "    \n",
    "alt.VConcatChart(vconcat=plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4426be8f-ef22-46aa-95b3-4c5ee243b659",
   "metadata": {},
   "source": [
    "# DeepVAR: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36b98a3-ba02-42bb-8460-87802361d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouper = MultivariateGrouper(max_target_dim=num_series)\n",
    "train_dataset = train_grouper(train_dataset)\n",
    "\n",
    "test_grouper = MultivariateGrouper(max_target_dim=num_series)\n",
    "test_dataset = test_grouper(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f616055-ff46-413f-b267-e61af09bbb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    ctx=device,\n",
    "    epochs=200,\n",
    "    num_batches_per_epoch=50,\n",
    "    learning_rate=0.001,\n",
    "    hybridize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32c52db-6eb8-43ae-b862-24704543728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepvar_estimator = DeepVAREstimator(\n",
    "    freq=\"D\", \n",
    "    prediction_length=test_steps,\n",
    "    target_dim=num_series,\n",
    "    trainer=trainer,\n",
    "    context_length=2 * test_steps,\n",
    "    num_layers=2,\n",
    "    num_cells=40,\n",
    "    cell_type=\"lstm\",\n",
    "    dropout_rate=0.2,\n",
    "    cardinality=cardinality,\n",
    "    distr_output=LowrankMultivariateGaussianOutput(dim=num_series, rank=5),\n",
    "    batch_size=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b550c-5463-4eaa-9456-efc49f1b7dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "deepvar_predictor = deepvar_estimator.train(train_dataset)\n",
    "\n",
    "os.makedirs(os.path.join(MODEL_PATH, \"deepvar\"), exist_ok=True)\n",
    "deepvar_predictor.serialize(Path(os.path.join(MODEL_PATH, \"deepvar\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c94554-75b9-495b-ad1e-1498009a1f56",
   "metadata": {},
   "source": [
    "# DeepVAR: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0cad13-e71f-4d9e-a09b-2548565ee8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepvar_predictor = Predictor.deserialize(Path(os.path.join(MODEL_PATH, \"deepvar\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5b9267-049b-4b3b-9023-aae28bcbec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "forecast_iter, ts_iter = make_evaluation_predictions( \n",
    "    dataset=test_dataset,\n",
    "    predictor=deepvar_predictor, \n",
    "    num_samples=100,\n",
    ") \n",
    "forecasts = list(forecast_iter)\n",
    "tss = list(ts_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b231b76d-97ce-4418-8532-65e3b56b5c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MultivariateEvaluator(quantiles=[0.1, 0.5, 0.9])\n",
    "agg_metrics = evaluator.calculate_aggregate_multivariate_metrics(\n",
    "    iter(tss), iter(forecasts), np.mean\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96be2e24-589c-4bac-bf44-c697f8b89753",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\n",
    "for key, value in agg_metrics.items():\n",
    "    if not np.isnan(value):\n",
    "        string += key + \": \" + f\"{value:.4f}\\n\"\n",
    "        \n",
    "print(string[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c565b0ec-521f-4bb7-a6b1-43dfa271da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [forecast.mean for forecast in forecasts]\n",
    "df_pred_sampled = pd.DataFrame(predictions[0].T, columns=df_test_sampled.columns)\n",
    "\n",
    "wrmsse = wrmsse_evaluator.score(df_pred_sampled)\n",
    "\n",
    "print(f\"DeepVAR WRMSSE: {wrmsse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06edae37-8a0c-4dcc-a505-d665cf4440a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_indices = [2, 4, 8]\n",
    "plots = []\n",
    "\n",
    "for plot_index in plot_indices:\n",
    "    plot_id = test_sampled_ids[plot_index]\n",
    "\n",
    "    y = target[[plot_id]].reset_index()\n",
    "    y.columns = [\"time\", \"y\"]\n",
    "    y[\"time\"] = pd.to_datetime(y[\"time\"])\n",
    "\n",
    "    forecast = pd.DataFrame(\n",
    "        [forecasts[0].mean[plot_index][-test_steps:]]\n",
    "        + [forecasts[0].quantile(p)[plot_index][-test_steps:] for p in [0.05, 0.25, 0.75, 0.95]],\n",
    "        columns=forecasts[0].index,\n",
    "    ).T.reset_index()\n",
    "    forecast.columns = [\n",
    "        \"time\",\n",
    "        \"fcst\",\n",
    "        \"fcst_lower_05\",\n",
    "        \"fcst_lower_25\",\n",
    "        \"fcst_upper_75\",\n",
    "        \"fcst_upper_95\",\n",
    "    ]\n",
    "\n",
    "    source = y.merge(forecast, how=\"left\")\n",
    "    p = plot_forecast(\n",
    "        source, test_steps, plot_id=plot_id, model_name=\"DeepVAR\", start_date=\"2015-05-23\"\n",
    "    )\n",
    "    \n",
    "    plots.append(p)\n",
    "    \n",
    "alt.VConcatChart(vconcat=plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3decfb94-68e3-43a0-9fa9-61522595226f",
   "metadata": {},
   "source": [
    "# DeepAR: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5488e7ec-c421-4ebe-86ec-0d64f9880fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n",
    "events = calendar[[\"date\"] + feature_names].fillna(\"NA\")\n",
    "train = events[events[\"date\"] < dates[-test_steps]][feature_names]\n",
    "\n",
    "encoder = HashingEncoder(drop_invariant=True)\n",
    "_ = encoder.fit(train)\n",
    "encoded = encoder.transform(events[feature_names])\n",
    "events = pd.concat([events[[\"date\"]], encoded], axis=1)\n",
    "\n",
    "feat_dynamic_cat = events.merge(time_related).set_index(\"date\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(feat_dynamic_cat)\n",
    "feat_dynamic_cat = pd.DataFrame(\n",
    "    scaled, columns=feat_dynamic_cat.columns, index=feat_dynamic_cat.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4599568-2a70-41a8-9578-28dd87ab7bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = (\n",
    "    df_train_eval[[\"id\", \"store_id\", \"item_id\"]]\n",
    "    .merge(selling_prices, how=\"left\")\n",
    "    .drop([\"store_id\", \"item_id\"], axis=1)\n",
    ")\n",
    "week_to_date = calendar[[\"date\", \"wm_yr_wk\"]].drop_duplicates()\n",
    "prices = week_to_date.merge(prices, how=\"left\").drop(\n",
    "    [\"wm_yr_wk\"], axis=1\n",
    ")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train = prices[prices[\"date\"] < dates[-test_steps]][[\"sell_price\"]]\n",
    "\n",
    "_ = scaler.fit(train)\n",
    "prices[\"sell_price\"] = scaler.transform(prices[[\"sell_price\"]])\n",
    "prices = prices.pivot(index=\"date\", columns=\"id\", values=\"sell_price\")\n",
    "prices = prices.fillna(method=\"bfill\")\n",
    "\n",
    "feat_dynamic_real = pd.concat([prices, snap], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d0ca7-6209-4301-8eb1-f11c7803eb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = []\n",
    "for each_id in all_ids:\n",
    "    dict_by_id = {\n",
    "        FieldName.TARGET: target[each_id].values,\n",
    "        FieldName.START: target.index[0],\n",
    "        FieldName.FEAT_DYNAMIC_REAL: split_into_n_array(\n",
    "            feat_dynamic_cat.iloc[:-test_steps],\n",
    "            n_feat_dynamic_cat,\n",
    "        )\n",
    "        + split_into_n_array(\n",
    "            feat_dynamic_real[each_id].iloc[:-test_steps],\n",
    "            n_feat_dynamic_real,\n",
    "        ),\n",
    "        FieldName.FEAT_STATIC_CAT: feat_static_cat[each_id].values,\n",
    "    }\n",
    "    train_list.append(dict_by_id)\n",
    "\n",
    "test_list = []\n",
    "for each_id in all_ids:\n",
    "    dict_by_id = {\n",
    "        FieldName.TARGET: np.append(\n",
    "            target[each_id].values, np.repeat(np.nan, test_steps)\n",
    "        ),\n",
    "        FieldName.START: target.index[0],\n",
    "        FieldName.FEAT_DYNAMIC_REAL: split_into_n_array(\n",
    "            feat_dynamic_cat,\n",
    "            n_feat_dynamic_cat,\n",
    "        )\n",
    "        + split_into_n_array(\n",
    "            feat_dynamic_real[each_id],\n",
    "            n_feat_dynamic_real,\n",
    "        ),\n",
    "        FieldName.FEAT_STATIC_CAT: feat_static_cat[each_id].values,\n",
    "    }\n",
    "    test_list.append(dict_by_id)\n",
    "\n",
    "train_dataset = ListDataset(train_list, freq=\"D\")\n",
    "test_dataset = ListDataset(test_list, freq=\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d6b7a-4c6c-47e1-bd5f-7b8ca1b0aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    ctx=device,\n",
    "    epochs=200,\n",
    "    num_batches_per_epoch=50,\n",
    "    learning_rate=0.001,\n",
    "    hybridize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11339b98-262e-4575-883a-a83e0d5e69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepar_estimator = DeepAREstimator(\n",
    "    freq=\"D\", \n",
    "    prediction_length=test_steps,\n",
    "    trainer=trainer,\n",
    "    context_length=2 * test_steps,\n",
    "    num_layers=2,\n",
    "    num_cells=40,\n",
    "    cell_type=\"lstm\",\n",
    "    dropout_rate=0.2,\n",
    "    use_feat_dynamic_real=True,\n",
    "    use_feat_static_cat=True,\n",
    "    use_feat_static_real=False,\n",
    "    cardinality=cardinality,\n",
    "    distr_output=NegativeBinomialOutput(),  \n",
    "    batch_size=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a3930c-7a7a-4bec-8efb-90a97758924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "deepar_predictor = deepar_estimator.train(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c2d95d-6766-4a24-98f0-cf4b1e217680",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "forecast_iter, ts_iter = make_evaluation_predictions( \n",
    "    dataset=test_dataset,\n",
    "    predictor=deepar_predictor, \n",
    "    num_samples=100,\n",
    ") \n",
    "forecasts = list(forecast_iter)\n",
    "tss = list(ts_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9cc279-f6f8-45f3-8c43-4adfa734f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"F\" + str(i) for i in range(1, 29)]\n",
    "valid_submission = df_train_eval.loc[\n",
    "    :, [\"id\"] + date_names[-test_steps:]\n",
    "]\n",
    "valid_submission.columns = [\"id\"] + column_names\n",
    "valid_submission[\"id\"] = valid_submission[\"id\"].str.replace(\"evaluation\", \"validation\")\n",
    "eval_submission = pd.DataFrame(\n",
    "    [forecast.mean for forecast in forecasts],\n",
    "    columns=column_names,\n",
    "    index=all_ids,\n",
    ").reset_index()\n",
    "eval_submission.columns = [\"id\"] + eval_submission.columns[1:].tolist()\n",
    "submission = pd.concat([valid_submission, eval_submission])\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
