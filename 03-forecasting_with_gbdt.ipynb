{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1121594-bd32-4723-be44-7486fbbe57ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f19a86b-a69a-4dc0-9abe-ce3b6321dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import multiprocessing\n",
    "import os\n",
    "import warnings\n",
    "import hyperopt\n",
    "import tsfresh\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import lightgbm as lgb\n",
    "from hyperopt import fmin, hp, space_eval, STATUS_OK, tpe, Trials\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_tweedie_deviance\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction.settings import (\n",
    "    EfficientFCParameters, \n",
    "    MinimalFCParameters,\n",
    ")\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series\n",
    "from utils.evaluation import calc_eval_metric, WRMSSEEvaluator\n",
    "from utils.misc import dump_pickle, load_pickle\n",
    "\n",
    "np.random.seed(42)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "n_jobs = multiprocessing.cpu_count() - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd41e9-420f-400a-8adb-5cf529c79b1a",
   "metadata": {},
   "source": [
    "The Kaggle dataset was saved in the local directory `~/data/mofc-demand-forecast` in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdefcfe-99a4-4217-bcb2-0bf92845c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../data/mofc-demand-forecast\"\n",
    "MODEL_PATH = \"models\"\n",
    "\n",
    "calendar = pd.read_csv(os.path.join(DATA_PATH, \"calendar.csv\"))\n",
    "selling_prices = pd.read_csv(os.path.join(DATA_PATH, \"sell_prices.csv\"))\n",
    "# df_train_valid = pd.read_csv(os.path.join(DATA_PATH, \"sales_train_validation.csv\"))\n",
    "df_train_eval = pd.read_csv(os.path.join(DATA_PATH, \"sales_train_evaluation.csv\"))\n",
    "# sample_submission = pd.read_csv(os.path.join(DATA_PATH, \"sample_submission.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cfc0db-1be8-4515-be2f-79669c02b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_ids = [\"id\", \"date\"]\n",
    "all_ids = df_train_eval[\"id\"].unique()\n",
    "date_names = [\"d_\" + str(i) for i in range(1, 1942)]\n",
    "calendar[\"date\"] = pd.to_datetime(calendar[\"date\"])\n",
    "dates = calendar[\"date\"].unique()\n",
    "test_steps = 28\n",
    "\n",
    "key_pairs = list(itertools.product(all_ids, dates))\n",
    "key_pairs = pd.DataFrame(key_pairs, columns=key_ids)\n",
    "\n",
    "sample_ratio = 0.1\n",
    "\n",
    "if sample_ratio == 1.0:\n",
    "    sampled_ids = all_ids\n",
    "else:\n",
    "    sampled_ids = np.random.choice(\n",
    "        all_ids, round(sample_ratio * len(all_ids)), replace=False\n",
    "    ).tolist()\n",
    "    \n",
    "print(\n",
    "    f\"{len(sampled_ids)} out of {len(all_ids)} IDs were selected for validation and testing.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c6b9d-ebdb-4936-9f68-cb77bcb85e49",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8abba8-ee9f-4d9c-a7fd-a23748ae069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = df_train_eval[[\"id\"] + date_names]\n",
    "date_dict = calendar[[\"date\", \"d\"]].set_index(\"d\").to_dict()[\"date\"]\n",
    "sales.columns = pd.Series(sales.columns).replace(date_dict)\n",
    "sales = pd.melt(\n",
    "    sales,\n",
    "    id_vars=\"id\",\n",
    "    value_vars=sales.columns[1:],\n",
    "    var_name=\"date\",\n",
    "    value_name=\"sales\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8b409-a978-4adc-b188-493071f8cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(lst, n):\n",
    "    q = len(lst) // n\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        if i == n - 1:\n",
    "            chunks.append(lst[q * i : len(lst)])\n",
    "        else:\n",
    "            chunks.append(lst[q * i : q * (i + 1)])\n",
    "            \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bacee4e-35f9-44e2-8aee-b32eef134669",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = date_dict[date_names[-test_steps]]\n",
    "sales_train = (\n",
    "    sales[sales[\"date\"] < split].set_index(\"id\").loc[sampled_ids].reset_index()\n",
    ")\n",
    "\n",
    "frequencies = [7, 30, 90, 365]\n",
    "default_fc_parameters = MinimalFCParameters()\n",
    "# default_fc_parameters = EfficientFCParameters()\n",
    "\n",
    "chunks = split_list(sampled_ids, 10)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    for j, frequency in enumerate(frequencies):\n",
    "        df_rolled = roll_time_series(\n",
    "            sales_train.set_index(\"id\").loc[chunk].reset_index(),\n",
    "            column_id=\"id\",\n",
    "            column_sort=\"date\",\n",
    "            max_timeshift=frequency,\n",
    "            min_timeshift=frequency,\n",
    "            n_jobs=n_jobs,\n",
    "            disable_progressbar=False,\n",
    "        )\n",
    "\n",
    "        df_extracted = extract_features(\n",
    "            df_rolled[key_ids + [\"sales\"]],\n",
    "            default_fc_parameters=default_fc_parameters,\n",
    "            column_id=\"id\",\n",
    "            column_sort=\"date\",\n",
    "            n_jobs=n_jobs,\n",
    "            pivot=True,\n",
    "        )\n",
    "\n",
    "        df_extracted.columns = df_extracted.columns + f\"__D{frequency}\"\n",
    "\n",
    "        if j == 0:\n",
    "            df_part = df_extracted\n",
    "        else:\n",
    "            df_part = df_part.merge(\n",
    "                df_extracted, left_index=True, right_index=True\n",
    "            )\n",
    "            \n",
    "    if i == 0:\n",
    "        feat_dynamic_real = df_part\n",
    "    else:\n",
    "        feat_dynamic_real = pd.concat([feat_dynamic_real, df_part])\n",
    "        \n",
    "    df_part.to_pickle(os.path.join(DATA_PATH, f\"feat_dynamic_real__{i}.pkl\"))\n",
    "\n",
    "feat_dynamic_real = feat_dynamic_real.reset_index()\n",
    "feat_dynamic_real.columns = key_ids + feat_dynamic_real.columns[2:].tolist()\n",
    "feat_dynamic_real = feat_dynamic_real.merge(sales_train, on=key_ids).rename(\n",
    "    {\"sales\": \"sales__D1\"}, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71861f7-104e-43c6-b5d3-14747c9f64de",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = (\n",
    "    df_train_eval[[\"id\", \"store_id\", \"item_id\"]]\n",
    "    .merge(selling_prices, how=\"left\")\n",
    "    .drop([\"store_id\", \"item_id\"], axis=1)\n",
    ")\n",
    "week_to_date = calendar[[\"date\", \"wm_yr_wk\"]].drop_duplicates()\n",
    "prices = week_to_date.merge(prices, how=\"left\").drop(\n",
    "    [\"wm_yr_wk\"], axis=1\n",
    ")\n",
    "\n",
    "snap = calendar[[\"date\", \"snap_CA\", \"snap_TX\", \"snap_WI\"]]\n",
    "snap.columns = [\"date\", \"CA\", \"TX\", \"WI\"]\n",
    "snap = pd.melt(\n",
    "    snap,\n",
    "    id_vars=\"date\",\n",
    "    value_vars=[\"CA\", \"TX\", \"WI\"],\n",
    "    var_name=\"state_id\",\n",
    "    value_name=\"snap\",\n",
    ")\n",
    "snap = key_pairs.merge(df_train_eval[[\"id\", \"state_id\"]], how=\"left\").merge(\n",
    "    snap, on=[\"date\", \"state_id\"], how=\"left\"\n",
    ").drop([\"state_id\"], axis=1)\n",
    "\n",
    "feat_dynamic_real = feat_dynamic_real.merge(prices, on=key_ids).merge(\n",
    "    snap, on=key_ids\n",
    ")\n",
    "\n",
    "num_feature_names = feat_dynamic_real.columns.difference(key_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a74b9be-0de4-4d24-88e8-aa476d1cf9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dynamic_cat = calendar[\n",
    "    [\n",
    "        \"date\",\n",
    "        \"wday\",\n",
    "        \"month\",\n",
    "        \"event_name_1\",\n",
    "        \"event_type_1\",\n",
    "        \"event_name_2\",\n",
    "        \"event_type_2\",\n",
    "    ]\n",
    "]\n",
    "feat_dynamic_cat[\"day\"] = (\n",
    "    feat_dynamic_cat[\"date\"].astype(\"str\").map(lambda x: int(x.split(\"-\")[2]))\n",
    ")\n",
    "\n",
    "feat_static_cat = df_train_eval[\n",
    "    [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "]\n",
    "\n",
    "cat_feature_names = feat_dynamic_cat.columns\n",
    "cat_feature_names = cat_feature_names.union(feat_static_cat.columns).difference(key_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a7f726-63eb-435a-8fa1-39f2172a299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = feat_dynamic_real.merge(feat_dynamic_cat).merge(feat_static_cat)\n",
    "df_train[\"date\"] = df_train[\"date\"].map(lambda x: x + pd.Timedelta(\"1 days\"))\n",
    "df_recur = df_train[df_train[\"date\"] == split].set_index(key_ids)\n",
    "df_train = df_train.merge(sales_train, on=key_ids)\n",
    "\n",
    "all_feature_names = num_feature_names.union(cat_feature_names)\n",
    "feature_names_to_remove = all_feature_names[\n",
    "    (df_train[all_feature_names].isna().sum() / df_train.shape[0] == 1.0)\n",
    "    | (df_train[all_feature_names].std() == 0.0)\n",
    "]\n",
    "num_feature_names = num_feature_names.difference(feature_names_to_remove)\n",
    "cat_feature_names = cat_feature_names.difference(feature_names_to_remove)\n",
    "all_feature_names = all_feature_names.difference(feature_names_to_remove)\n",
    "\n",
    "train_dates = df_train[\"date\"].unique()\n",
    "df_x_train = (\n",
    "    df_train[key_ids + all_feature_names.difference([\"sales\"]).tolist()]\n",
    "    .set_index(key_ids)\n",
    "    .swaplevel()\n",
    ")\n",
    "df_y_train = df_train[key_ids + [\"sales\"]].set_index(key_ids).swaplevel()\n",
    "\n",
    "df_train.to_pickle(os.path.join(DATA_PATH, \"df_train.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41561f3-3122-4573-894d-1422e4049d9f",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39446f59-9bc5-4a4e-95e9-48ec77514e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(args):\n",
    "    global n_jobs\n",
    "    global train_dates\n",
    "    global df_x_train, df_y_train\n",
    "    global all_feature_names, num_feature_names, cat_feature_names\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    cat_pipeline = make_pipeline(\n",
    "        SimpleImputer(strategy=\"constant\", fill_value=\"<unknown>\"),\n",
    "        OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1),\n",
    "    )\n",
    "    num_pipeline = SimpleImputer(strategy=\"median\")\n",
    "    processor = make_column_transformer(\n",
    "        (cat_pipeline, cat_feature_names), (num_pipeline, num_feature_names)\n",
    "    )\n",
    "\n",
    "    default_params = {\n",
    "        \"objective\": \"tweedie\",\n",
    "        \"num_threads\": n_jobs,\n",
    "        \"device_type\": \"cpu\",\n",
    "        \"seed\": 42,\n",
    "        \"max_cat_threshold\": 32,\n",
    "        \"verbosity\": 1,\n",
    "        \"max_bin_by_feature\": None,\n",
    "        \"min_data_in_bin\": 3,\n",
    "        \"feature_pre_filter\": True,\n",
    "        \"tweedie_variance_power\": 1.5,\n",
    "        \"metric\": \"tweedie\",\n",
    "    }\n",
    "    params = {\n",
    "        \"boosting\": args[\"boosting\"],\n",
    "        \"learning_rate\": args[\"learning_rate\"],\n",
    "        \"num_iterations\": int(args[\"num_iterations\"]),\n",
    "        \"num_leaves\": int(args[\"num_leaves\"]),\n",
    "        \"max_depth\": int(args[\"max_depth\"]),\n",
    "        \"min_data_in_leaf\": int(args[\"min_data_in_leaf\"]),\n",
    "        \"min_sum_hessian_in_leaf\": args[\"min_sum_hessian_in_leaf\"],\n",
    "        \"bagging_fraction\": args[\"bagging_fraction\"],\n",
    "        \"bagging_freq\": int(args[\"bagging_freq\"]),\n",
    "        \"feature_fraction\": args[\"feature_fraction\"],\n",
    "        \"extra_trees\": args[\"extra_trees\"],\n",
    "        \"lambda_l1\": args[\"lambda_l1\"],\n",
    "        \"lambda_l2\": args[\"lambda_l2\"],\n",
    "        \"path_smooth\": args[\"path_smooth\"],\n",
    "        \"max_bin\": int(args[\"max_bin\"]),\n",
    "    }\n",
    "    default_params.update(params)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for train_index, test_index in tscv.split(train_dates):\n",
    "        dtrain = df_x_train.loc[train_dates[train_index], :]\n",
    "        dvalid = df_x_train.loc[train_dates[test_index], :]\n",
    "\n",
    "        dtrain = processor.fit_transform(dtrain)\n",
    "        dvalid = processor.transform(dvalid)\n",
    "        dtrain = lgb.Dataset(dtrain, label=df_y_train.loc[train_dates[train_index], :])\n",
    "\n",
    "        model = lgb.train(\n",
    "            default_params,\n",
    "            dtrain,\n",
    "            feature_name=all_feature_names.tolist(),\n",
    "            categorical_feature=cat_feature_names.tolist(),\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "\n",
    "        y_true = df_y_train.loc[train_dates[test_index], :]\n",
    "        y_pred = model.predict(dvalid)\n",
    "        loss = mean_tweedie_deviance(\n",
    "            y_true, y_pred, power=default_params[\"tweedie_variance_power\"]\n",
    "        )\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "    return {\"loss\": np.mean(losses), \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a057f3-ae21-4135-b914-76342a8a06a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    \"boosting\": hp.pchoice(\"boosting\", [(0.75, \"gbdt\"), (0.25, \"dart\")]),\n",
    "    \"learning_rate\": 10 ** hp.uniform(\"learning_rate\", -2, 0),\n",
    "    \"num_iterations\": hp.quniform(\"num_iterations\", 1, 1000, 1),\n",
    "    \"num_leaves\": 2 ** hp.uniform(\"num_leaves\", 1, 8),\n",
    "    \"max_depth\": -1,\n",
    "    \"min_data_in_leaf\": 2 * 10 ** hp.uniform(\"min_data_in_leaf\", 0, 2),\n",
    "    \"min_sum_hessian_in_leaf\": hp.uniform(\"min_sum_hessian_in_leaf\", 1e-4, 1e-2),\n",
    "    \"bagging_fraction\": hp.uniform(\"bagging_fraction\", 0.5, 1.0),\n",
    "    \"bagging_freq\": hp.qlognormal(\"bagging_freq\", 0.0, 1.0, 1),\n",
    "    \"feature_fraction\": hp.uniform(\"feature_fraction\", 0.5, 1.0),\n",
    "    \"extra_trees\": hp.pchoice(\"extra_trees\", [(0.75, False), (0.25, True)]),\n",
    "    \"lambda_l1\": hp.lognormal(\"lambda_l1\", 0.0, 1.0),\n",
    "    \"lambda_l2\": hp.lognormal(\"lambda_l2\", 0.0, 1.0),\n",
    "    \"path_smooth\": hp.lognormal(\"path_smooth\", 0.0, 1.0),\n",
    "    \"max_bin\": 2 ** hp.quniform(\"max_bin\", 6, 10, 1) - 1,\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(\n",
    "    objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials,\n",
    ")\n",
    "\n",
    "best_params = space_eval(space, best)\n",
    "best_params[\"num_iterations\"] = int(best_params[\"num_iterations\"])\n",
    "best_params[\"num_leaves\"] = int(best_params[\"num_leaves\"])\n",
    "best_params[\"min_data_in_leaf\"] = int(best_params[\"min_data_in_leaf\"])\n",
    "best_params[\"bagging_freq\"] = int(best_params[\"bagging_freq\"])\n",
    "best_params[\"max_bin\"] = int(best_params[\"max_bin\"])\n",
    "\n",
    "dump_pickle(os.path.join(MODEL_PATH, \"lgb\", \"best_params.pkl\"), best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d416824f-a790-4274-9d81-568217e9e571",
   "metadata": {},
   "source": [
    "# Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d6bf2-4856-48b4-abcf-a90cbe60af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"<unknown>\"),\n",
    "    OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1),\n",
    ")\n",
    "num_pipeline = SimpleImputer(strategy=\"median\")\n",
    "processor = make_column_transformer(\n",
    "    (cat_pipeline, cat_feature_names), (num_pipeline, num_feature_names)\n",
    ")\n",
    "\n",
    "default_params = {\n",
    "    \"objective\": \"tweedie\",\n",
    "    \"num_threads\": n_jobs,\n",
    "    \"device_type\": \"cpu\",\n",
    "    \"seed\": 42,\n",
    "    \"max_cat_threshold\": 32,\n",
    "    \"verbosity\": 1,\n",
    "    \"max_bin_by_feature\": None,\n",
    "    \"min_data_in_bin\": 3,\n",
    "    \"feature_pre_filter\": True,\n",
    "    \"tweedie_variance_power\": 1.5,\n",
    "    \"metric\": \"tweedie\",\n",
    "}\n",
    "default_params.update(best_params)\n",
    "\n",
    "dtrain = processor.fit_transform(df_x_train)\n",
    "dtrain = lgb.Dataset(dtrain, label=df_y_train)\n",
    "\n",
    "model = lgb.train(\n",
    "    default_params,\n",
    "    dtrain,\n",
    "    feature_name=all_feature_names.tolist(),\n",
    "    categorical_feature=cat_feature_names.tolist(),\n",
    "    verbose_eval=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f8251-91b6-4c5d-96aa-4fad041d66e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_recur\n",
    "sales_pred = sales_train\n",
    "\n",
    "for i in range(test_steps):\n",
    "    dtest = processor.transform(df_test[all_feature_names])\n",
    "    y_pred = model.predict(dtest)\n",
    "\n",
    "    sales_recur = pd.DataFrame(\n",
    "        y_pred, columns=[\"sales\"], index=df_test.index\n",
    "    ).reset_index()\n",
    "    sales_pred = pd.concat([sales_pred, sales_recur])\n",
    "    pred_dates = sales_pred[\"date\"].unique()\n",
    "\n",
    "    for j, frequency in enumerate(frequencies):\n",
    "        df_extracted = extract_features(\n",
    "            sales_pred.set_index(\"date\").loc[pred_dates[-frequency:], :].reset_index(),\n",
    "            default_fc_parameters=default_fc_parameters,\n",
    "            column_id=\"id\",\n",
    "            column_sort=\"date\",\n",
    "            n_jobs=n_jobs,\n",
    "            disable_progressbar=False,\n",
    "        )\n",
    "\n",
    "        df_extracted.columns = df_extracted.columns + f\"__D{frequency}\"\n",
    "\n",
    "        if j == 0:\n",
    "            feat_dynamic_real = df_extracted\n",
    "        else:\n",
    "            feat_dynamic_real = feat_dynamic_real.merge(\n",
    "                df_extracted, left_index=True, right_index=True\n",
    "            )\n",
    "\n",
    "    feat_dynamic_real = feat_dynamic_real.reset_index()\n",
    "    feat_dynamic_real.columns = [\"id\"] + feat_dynamic_real.columns[1:].tolist()\n",
    "    feat_dynamic_real = feat_dynamic_real.merge(sales_recur[[\"id\", \"sales\"]]).rename(\n",
    "        {\"sales\": \"sales__D1\"}, axis=1\n",
    "    )\n",
    "    feat_dynamic_real[\"date\"] = pred_dates[-1] + pd.Timedelta(\"1 days\")\n",
    "\n",
    "    feat_dynamic_real = feat_dynamic_real.merge(prices, on=key_ids).merge(\n",
    "        snap, on=key_ids\n",
    "    )\n",
    "\n",
    "    df_test = (\n",
    "        feat_dynamic_real.merge(feat_dynamic_cat)\n",
    "        .merge(feat_static_cat)\n",
    "        .set_index(key_ids)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11176f77-bbd3-461d-94ee-99a69163c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_names = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "df_sampled = df_train_eval.set_index(\"id\").loc[sampled_ids].reset_index()\n",
    "df_train_sampled = df_sampled.loc[:, key_names + date_names[:-test_steps]]\n",
    "df_test_sampled = df_sampled.loc[:, date_names[-test_steps:]]\n",
    "\n",
    "evaluator = WRMSSEEvaluator(\n",
    "    df_train_sampled, df_test_sampled, calendar, selling_prices, test_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ec3c9-2033-430a-a75d-17243deddfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sales_pred[sales_pred[\"date\"] > dates[-2 * test_steps - 1]]\n",
    "predictions = predictions.pivot(index=\"date\", columns=\"id\", values=\"sales\")\n",
    "\n",
    "df_pred_sampled = predictions.T\n",
    "df_pred_sampled = df_pred_sampled.loc[sampled_ids]\n",
    "df_pred_sampled.columns = df_test_sampled.columns\n",
    "df_pred_sampled.index = range(len(sampled_ids))\n",
    "\n",
    "wrmsse = evaluator.score(df_pred_sampled)\n",
    "eval_metrics = calc_eval_metric(df_test_sampled, df_pred_sampled)\n",
    "\n",
    "print(f\"LightGBM WRMSSE: {wrmsse:.6f}\")\n",
    "display(eval_metrics.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d63c6-390d-473c-9b86-50fcc1e3ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecast(source, test_steps, plot_id=None, model_name=None, start_date=None):\n",
    "    if start_date is not None:\n",
    "        source = source[source[\"time\"] >= start_date]\n",
    "\n",
    "    points = (\n",
    "        alt.Chart(source)\n",
    "        .mark_circle(size=10.0, color=\"#000000\")\n",
    "        .encode(\n",
    "            x=alt.X(\"time:T\", axis=alt.Axis(title=\"Date\")),\n",
    "            y=alt.Y(\"y\", axis=alt.Axis(title=\"Demand\")),\n",
    "            tooltip=[\"time:T\", \"y:Q\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    line = (\n",
    "        alt.Chart(source)\n",
    "        .mark_line(size=1.0, color=\"#4267B2\")\n",
    "        .encode(\n",
    "            x=\"time:T\",\n",
    "            y=\"fcst\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    band = (\n",
    "        alt.Chart(source)\n",
    "        .mark_area(opacity=0.25, color=\"#4267B2\")\n",
    "        .encode(\n",
    "            x=\"time:T\",\n",
    "            y=\"fcst_lower\",\n",
    "            y2=\"fcst_upper\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    rule = (\n",
    "        alt.Chart(source[[\"time\"]].iloc[-test_steps : -test_steps + 1])\n",
    "        .mark_rule(size=1.0, color=\"#FF0000\", strokeDash=[2, 2])\n",
    "        .encode(x=\"time:T\")\n",
    "    )\n",
    "\n",
    "    title = \"Demand Forecast\"\n",
    "    if plot_id is not None:\n",
    "        title += f\" for '{plot_id}'\"\n",
    "    if model_name is not None:\n",
    "        title = f\"{model_name}: \" + title\n",
    "\n",
    "    return (points + line + band + rule).properties(title=title, width=1000, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3708db-0776-4a5f-b9ce-79e37d70efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_indices = [2, 4, 8]\n",
    "plots = []\n",
    "\n",
    "for plot_index in plot_indices:\n",
    "    plot_id = sampled_ids[plot_index]\n",
    "\n",
    "    y = (df_train_eval[df_train_eval[\"id\"] == plot_id].loc[:, date_names]).T\n",
    "    y.columns = [\"y\"]\n",
    "    y = calendar.merge(y, left_on=\"d\", right_index=True)[[\"date\", \"y\"]]\n",
    "    y[\"time\"] = pd.to_datetime(y[\"date\"])\n",
    "\n",
    "    source = y.merge(predictions[plot_id].reset_index(), how=\"left\").drop([\"date\"], axis=1)\n",
    "    source.columns = [\"y\", \"time\", \"fcst\"]\n",
    "    source[\"fcst_lower\"] = np.nan\n",
    "    source[\"fcst_upper\"] = np.nan\n",
    "\n",
    "    p = plot_forecast(\n",
    "        source, test_steps, plot_id=plot_id, model_name=\"LightGBM\", start_date=\"2015-05-23\"\n",
    "    )\n",
    "    \n",
    "alt.VConcatChart(vconcat=plots)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
